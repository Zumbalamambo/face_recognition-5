{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import inception_resnet_v1\n",
    "import tensorflow as tf\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../gender_age_tf/models/model.ckpt-14001\n",
      "restore model!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "images_pl = tf.placeholder(tf.float32, shape=[None, 160, 160, 3], name='input_image')\n",
    "images_norm = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame), images_pl)\n",
    "\n",
    "train_mode = tf.placeholder(tf.bool)\n",
    "\n",
    "age_logits, gender_logits, _ = inception_resnet_v1.inference(images_norm,\n",
    "                                                            keep_probability=0.8,\n",
    "                                                            phase_train=train_mode,\n",
    "                                                            weight_decay=1e-5)\n",
    "\n",
    "# Predict gender and age\n",
    "gender = tf.argmax(tf.nn.softmax(gender_logits), 1)\n",
    "age_ = tf.cast(tf.constant([i for i in range(0, 101)]), tf.float32)\n",
    "age = tf.reduce_sum(tf.multiply(tf.nn.softmax(age_logits), age_), axis=1)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                  tf.local_variables_initializer())\n",
    "\n",
    "sess.run(init_op)\n",
    "saver = tf.train.Saver()\n",
    "ckpt = tf.train.get_checkpoint_state(\"../gender_age_tf/models/\")\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"restore model!\")\n",
    "else:\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/damvantai/Desktop/ss/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(image_path + \"Ba1MxF8A6sF.1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/damvantai/Desktop/nguoidan/Ba1MxF8A6sF.1.jpg'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path + \"Ba1MxF8A6sF.1.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(image_path + \"Ba1MxF8A6sF.1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"/home/damvantai/Desktop/faces/1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 129, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected = detector(img, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rectangles[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               font_scale=1, thickness=2):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_girl = 1\n",
    "for i in os.listdir(image_path):\n",
    "#     print(i)\n",
    "    img = cv2.imread(image_path + i)\n",
    "#     print(img.shape)\n",
    "    input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     img_h, img_w, _ = np.shape(input_img)\n",
    "    \n",
    "    \n",
    "    detected = detector(input_img, 1)\n",
    "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "    \n",
    "    for i, d in enumerate(detected):\n",
    "        faces[i, :, :, :] = fa.align(input_img, gray, detected[i])\n",
    "        \n",
    "    if len(detected) > 0:\n",
    "        ages, genders = sess.run([age, gender], feed_dict={images_pl: faces, train_mode: False})\n",
    "    \n",
    "    \n",
    "    for i, d in enumerate(detected):\n",
    "        label = \"{}, {}\".format(int(ages[i]), \"F\" if genders[i] == 0 else \"M\")\n",
    "#         draw_label(img, (d.left(), d.top()), label)\n",
    "        if (genders[i]) == 0:\n",
    "#           \n",
    "            face_girl = img[d.top()-20:d.bottom()+20, d.left()-20:d.right()+20]\n",
    "            \n",
    "            cv2.imwrite(\"/home/damvantai/Desktop/faces/{}.jpg\".format(num_girl),face_girl)\n",
    "            num_girl += 1\n",
    "#     plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect girl from multi folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import inception_resnet_v1\n",
    "import tensorflow as tf\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../gender_age_tf/models/model.ckpt-14001\n",
      "restore model!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "images_pl = tf.placeholder(tf.float32, shape=[None, 160, 160, 3], name='input_image')\n",
    "images_norm = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame), images_pl)\n",
    "\n",
    "train_mode = tf.placeholder(tf.bool)\n",
    "\n",
    "age_logits, gender_logits, _ = inception_resnet_v1.inference(images_norm,\n",
    "                                                            keep_probability=0.8,\n",
    "                                                            phase_train=train_mode,\n",
    "                                                            weight_decay=1e-5)\n",
    "\n",
    "# Predict gender and age\n",
    "gender = tf.argmax(tf.nn.softmax(gender_logits), 1)\n",
    "age_ = tf.cast(tf.constant([i for i in range(0, 101)]), tf.float32)\n",
    "age = tf.reduce_sum(tf.multiply(tf.nn.softmax(age_logits), age_), axis=1)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                  tf.local_variables_initializer())\n",
    "\n",
    "sess.run(init_op)\n",
    "saver = tf.train.Saver()\n",
    "ckpt = tf.train.get_checkpoint_state(\"../gender_age_tf/models/\")\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"restore model!\")\n",
    "else:\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/damvantai/Desktop/faces/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/damvantai/Desktop/faces/1/\n",
      "/home/damvantai/Desktop/faces/2/\n",
      "/home/damvantai/Desktop/faces/3/\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir(image_path):\n",
    "    num_girl = 1\n",
    "    image_path_child = image_path + i + \"/\"\n",
    "    print(image_path_child)\n",
    "    for j in os.listdir(image_path_child):\n",
    "        img = cv2.imread(image_path_child + j)\n",
    "        input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        detected = detector(input_img, 1)\n",
    "        faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "        \n",
    "        for index, d in enumerate(detected):\n",
    "            faces[index, :, :, :] = fa.align(input_img, gray, detected[index])\n",
    "        \n",
    "        if len(detected) == 1:\n",
    "            ages, genders = sess.run([age, gender], feed_dict={images_pl: faces, train_mode: False})\n",
    "            if genders[0] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                os.remove(image_path_child + j)\n",
    "        else:\n",
    "            os.remove(image_path_child + j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_girl = 1\n",
    "for i in os.listdir(image_path):\n",
    "#     print(i)\n",
    "    img = cv2.imread(image_path + i)\n",
    "#     print(img.shape)\n",
    "    input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     img_h, img_w, _ = np.shape(input_img)\n",
    "    \n",
    "    \n",
    "    detected = detector(input_img, 1)\n",
    "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "    \n",
    "    for i, d in enumerate(detected):\n",
    "        faces[i, :, :, :] = fa.align(input_img, gray, detected[i])\n",
    "        \n",
    "    if len(detected) > 0:\n",
    "        ages, genders = sess.run([age, gender], feed_dict={images_pl: faces, train_mode: False})\n",
    "        \n",
    "    \n",
    "    for i, d in enumerate(detected):\n",
    "        label = \"{}, {}\".format(int(ages[i]), \"F\" if genders[i] == 0 else \"M\")\n",
    "#         draw_label(img, (d.left(), d.top()), label)\n",
    "        if (genders[i]) == 0:\n",
    "#           \n",
    "            face_girl = img[d.top()-20:d.bottom()+20, d.left()-20:d.right()+20]\n",
    "            \n",
    "            cv2.imwrite(\"/home/damvantai/Desktop/faces/{}.jpg\".format(num_girl),face_girl)\n",
    "            num_girl += 1\n",
    "#     plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
