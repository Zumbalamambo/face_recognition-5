{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import argparse\n",
    "import inception_resnet_v1\n",
    "import tensorflow as tf\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "import glob\n",
    "import re\n",
    "import ntpath\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rect_to_css(rect):\n",
    "    \"\"\"\n",
    "    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n",
    "    \n",
    "    :Param rect: a dlib 'rect' object\n",
    "    :Return: a plain tuple representation of the rect in (top, right, bottom, left) order\n",
    "    \"\"\"\n",
    "    return rect.top(), rect.right(), rect.bottom(), rect.left()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "              font_scale=1, thickness=2):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sess,age,gender,train_mode,images_pl):\n",
    "#     # list name file\n",
    "#     image_list_file = []\n",
    "    \n",
    "#     # collect all file in folder to list filename\n",
    "#     for filename in glob.glob('/home/damvantai/Documents/face_recognition_demo/pictures_of_people_i_know/*'):\n",
    "# #         print(filename)\n",
    "#         image_list_file.append(filename)\n",
    "    \n",
    "#     # Show name person in image\n",
    "#     image_names = []\n",
    "#     image_names = [ntpath.split(name)[1].split('.')[0] for name in image_list_file]\n",
    "    \n",
    "#     # list face_encodings and list face name\n",
    "#     known_face_encodings = []\n",
    "#     known_face_names = []\n",
    "    \n",
    "    # Load known_face_encoding_array and known_face name from npy\n",
    "    known_face_encodings_array = np.load(\"../data/numpy/known_face_encoding.npy\")\n",
    "    known_face_names = np.load(\"../data/numpy/known_face_names.npy\")\n",
    "\n",
    "    # Convert nparray -> list \n",
    "    number_person = len(known_face_encodings_array)\n",
    "    known_face_encodings_array = known_face_encodings_array.reshape(number_person, 128)\n",
    "    known_face_encodings = []\n",
    "    for i in range(len(known_face_encodings_array)):\n",
    "        known_face_encodings.append(known_face_encodings_array[i])\n",
    "        \n",
    "        \n",
    "    # Load data from data (known_face_encodings and known_face_name)\n",
    "#     with open(\"../data/data_face_encodings.txt\", \"rb\") as f:\n",
    "#         known_face_encodings = pickle.load(f)\n",
    "#     f.close\n",
    "    \n",
    "#     with open(\"../data/data_face_name.txt\", \"rb\") as f:\n",
    "#         known_face_names = pickle.load(f)\n",
    "#     f.close\n",
    "    \n",
    "    # for face detection\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    fa = FaceAligner(predictor, desiredFaceWidth=160)\n",
    "    \n",
    "#     for i, filename in enumerate(image_list_file):\n",
    "#         # Create array image\n",
    "#         image = face_recognition.load_image_file(image_list_file[i])\n",
    "        \n",
    "#         # encoding vector embedding for ever person\n",
    "#         known_face_encodings.append(face_recognition.face_encodings(image)[0])\n",
    "#         known_face_names.append(image_names[i])\n",
    "        \n",
    "    \n",
    "    # Initialize some variables\n",
    "    depth = 16\n",
    "    k = 8\n",
    "    frame_number = 0\n",
    "    face_locations = []\n",
    "    face_encodings = []\n",
    "    face_names = []\n",
    "    process_this_frame = True\n",
    "    name = \"\"\n",
    "    count = 0\n",
    "    img_size = 160\n",
    "\n",
    "    # capture video\n",
    "#     cap = cv2.VideoCapture(1)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "#     cap = cv2.VideoCapture('/home/damvantai/Documents/face_recognition_demo/phodibo_hoankiem.mp4')\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#     out = cv2.VideoWriter('output1.avi',fourcc, 4.0, (640,480))\n",
    "    \n",
    "    while True:\n",
    "        number_person = 0\n",
    "        number_male = 0\n",
    "        number_female = 0\n",
    "        \n",
    "        # get video frame\n",
    "        ret, img = cap.read()\n",
    "        \n",
    "        # video no exist\n",
    "        if not ret:\n",
    "            print(\"error: failed to capture image\")\n",
    "            return -1\n",
    "        \n",
    "        # increase ever frame\n",
    "        frame_number += 1\n",
    "        \n",
    "        # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "#         small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        \n",
    "        # Convert the image from BGR color (which opencv user) to RGB color (which face_recognition uses)\n",
    "        rgb_small_frame = img[:, :, ::-1]\n",
    "#         input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#         img_h, img_w, _ = np.shape(input_img)\n",
    "        \n",
    "#         detect faces using dlib detector\n",
    "        detected = detector(rgb_small_frame, 1)\n",
    "        faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "        \n",
    "        \n",
    "        # save frame image\n",
    "#         if (frame_number % 30 == 0)\n",
    "#             cv2.imwrite(\"/home/damvantai/Documents/face_recognition_demo/screenshot/frame%d.jpg\" %count, img)\n",
    "#             count += 1\n",
    "        \n",
    "        # ever 20 frame do\n",
    "        if (frame_number % 10 == 0):\n",
    "            # Only process every other frame of video to save time\n",
    "            if process_this_frame:\n",
    "                # Convert detected_rect - face location\n",
    "                face_locations = []\n",
    "                for i in detected:\n",
    "                    face_locations.append(_rect_to_css(i))\n",
    "\n",
    "                face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "#                 face_encodings = face_recognition.face_encodings(rgb_small_frame)\n",
    "                face_names = []\n",
    "                for face_encoding in face_encodings:\n",
    "                    # See if the face is a match for the known face(s)\n",
    "                    matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.42)\n",
    "                    print(matches)\n",
    "                    print(np.min(face_recognition.face_distance(known_face_encodings, face_encoding)), name)             \n",
    "                    name = \"Unknown\"\n",
    "                    # If a match was found in known_face_encodings, just use the first one.\n",
    "                    if True in matches:\n",
    "                        first_match_index = matches.index(True)\n",
    "                        name = known_face_names[first_match_index]\n",
    "\n",
    "                    face_names.append(name)\n",
    "        \n",
    "            \n",
    "        for i, d in enumerate(detected):\n",
    "            x1, y1, x2, y2= d.left(), d.top(), d.right() + 1, d.bottom() + 1\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            faces[i, :, :, :] = fa.align(rgb_small_frame, gray, detected[i])\n",
    "            \n",
    "        #\n",
    "        if len(detected) > 0:\n",
    "            # predict ages and genders of the detected faces\n",
    "            ages,genders = sess.run([age, gender], feed_dict={images_pl: faces, train_mode: False})\n",
    "    \n",
    "        # draw results\n",
    "#         for i, d in enumerate(face_locations):\n",
    "        for i, d in enumerate(detected):\n",
    "            if genders[i] != 0:\n",
    "                number_male += 1\n",
    "            label = \"{}, {}\".format(int(ages[i]), \"F\" if genders[i] == 0 else \"M\")\n",
    "#             draw_label(img, (d.left(), d.top()), label)\n",
    "#             draw_label(img, (d[0], d[1], label))\n",
    "            draw_label(img, (d.left(), d.top()), label)\n",
    "        \n",
    "        # Show number person and male/female\n",
    "        number_person = len(detected)\n",
    "        number_female = number_person - number_male\n",
    "#         print(number_person)\n",
    "#         print(\"Female {}\".format(number_female))\n",
    "#         print(\"Male {}\".format(number_male))\n",
    "        \n",
    "#         # Display the results\n",
    "#         for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "# #             # Draw a box around the face\n",
    "# #             cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "            \n",
    "#             # Draw a label with a name below the face\n",
    "#             cv2.rectangle(img, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#             font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#             cv2.putText(img, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "        \n",
    "         # Display the results\n",
    "        for d, name in zip(detected, face_names):\n",
    "#             # Draw a box around the face\n",
    "#             cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "            \n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(img, (d.left(), d.bottom() - 35), (d.right(), d.bottom()), (0, 0, 255), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(img, name, (d.left() + 6, d.bottom() - 6), font, 1.0, (255, 255, 255), 1)\n",
    "        \n",
    "        \n",
    "        cv2.putText(img, \"Male: {}\".format(number_male), (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(img, \"Female: {}\".format(number_female), (300, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "#         out.write(img)\n",
    "        \n",
    "        cv2.imshow(\"result\", img)\n",
    "        key = cv2.waitKey(1)\n",
    "\n",
    "        if key == 27:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(sess,age,gender,train_mode,images_pl):\n",
    "#     # list name file\n",
    "#     image_list_file = []\n",
    "    \n",
    "#     # collect all file in folder to list filename\n",
    "#     for filename in glob.glob('/home/damvantai/Documents/face_recognition_demo/pictures_of_people_i_know/*'):\n",
    "# #         print(filename)\n",
    "#         image_list_file.append(filename)\n",
    "    \n",
    "#     # Show name person in image\n",
    "#     image_names = []\n",
    "#     image_names = [ntpath.split(name)[1].split('.')[0] for name in image_list_file]\n",
    "    \n",
    "#     # list face_encodings and list face name\n",
    "#     known_face_encodings = []\n",
    "#     known_face_names = []\n",
    "    \n",
    "#     for i, filename in enumerate(image_list_file):\n",
    "#         # Create array image\n",
    "#         image = face_recognition.load_image_file(image_list_file[i])\n",
    "#         # encoding vector embedding for ever person\n",
    "#         known_face_encodings.append(face_recognition.face_encodings(image)[0])\n",
    "#         known_face_names.append(image_names[i])\n",
    "        \n",
    "    \n",
    "#     # Initialize some variables\n",
    "#     depth = 16\n",
    "#     k = 8\n",
    "#     frame_number = 0\n",
    "#     face_locations = []\n",
    "#     face_encodings = []\n",
    "#     face_names = []\n",
    "#     process_this_frame = True\n",
    "#     name = \"\"\n",
    "#     count = 0\n",
    "    \n",
    "#     # for face detection\n",
    "#     detector = dlib.get_frontal_face_detector()\n",
    "#     predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "#     fa = FaceAligner(predictor, desiredFaceWidth=160)\n",
    "\n",
    "#     img_size = 160\n",
    "\n",
    "#     # capture video\n",
    "# #     cap = cv2.VideoCapture(1)\n",
    "#     cap = cv2.VideoCapture(0)\n",
    "# #     cap = cv2.VideoCapture('/home/damvantai/Documents/face_recognition_demo/phodibo_hoankiem.mp4')\n",
    "    \n",
    "#     # Define the codec and create VideoWriter object\n",
    "# #     fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "# #     out = cv2.VideoWriter('output1.avi',fourcc, 4.0, (640,480))\n",
    "    \n",
    "#     while True:\n",
    "#         number_person = 0\n",
    "#         number_male = 0\n",
    "#         number_female = 0\n",
    "        \n",
    "#         # get video frame\n",
    "#         ret, img = cap.read()\n",
    "        \n",
    "#         # video no exist\n",
    "#         if not ret:\n",
    "#             print(\"error: failed to capture image\")\n",
    "#             return -1\n",
    "        \n",
    "#         # increase ever frame\n",
    "#         frame_number += 1\n",
    "        \n",
    "#         # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "# #         small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        \n",
    "#         # Convert the image from BGR color (which opencv user) to RGB color (which face_recognition uses)\n",
    "#         rgb_small_frame = img[:, :, ::-1]\n",
    "# #         input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# #         img_h, img_w, _ = np.shape(input_img)\n",
    "        \n",
    "#         # detect faces using dlib detector\n",
    "#         detected = detector(rgb_small_frame, 1)\n",
    "#         faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "        \n",
    "        \n",
    "#         # save frame image\n",
    "# #         if (frame_number % 30 == 0)\n",
    "# #             cv2.imwrite(\"/home/damvantai/Documents/face_recognition_demo/screenshot/frame%d.jpg\" %count, img)\n",
    "# #             count += 1\n",
    "        \n",
    "#         # ever 20 frame do\n",
    "#         if (frame_number % 10 == 0):\n",
    "#             # Only process every other frame of video to save time\n",
    "#             if process_this_frame:\n",
    "#                 # Convert detected_rect - face location\n",
    "# #                 face_locations = []\n",
    "# #                 for i in detected:\n",
    "# #                     face_locations.append(_rect_to_css(i))\n",
    "\n",
    "# #                 face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "#                 face_encodings = face_recognition.face_encodings(rgb_small_frame)\n",
    "#                 face_names = []\n",
    "#                 for face_encoding in face_encodings:\n",
    "#                     # See if the face is a match for the known face(s)\n",
    "#                     matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.4)\n",
    "                    \n",
    "#                     print(np.min(face_recognition.face_distance(known_face_encodings, face_encoding)), name)             \n",
    "#                     name = \"Unknown\"\n",
    "#                     # If a match was found in known_face_encodings, just use the first one.\n",
    "#                     if True in matches:\n",
    "#                         first_match_index = matches.index(True)\n",
    "#                         name = known_face_names[first_match_index]\n",
    "\n",
    "#                     face_names.append(name)\n",
    "        \n",
    "\n",
    "        \n",
    "#         for i, d in enumerate(detected):\n",
    "#             x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "#             cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "#             faces[i, :, :, :] = fa.align(rgb_small_frame, gray, detected[i])\n",
    "            \n",
    "#         #\n",
    "#         if len(detected) > 0:\n",
    "#             # predict ages and genders of the detected faces\n",
    "#             ages,genders = sess.run([age, gender], feed_dict={images_pl: faces, train_mode: False})\n",
    "    \n",
    "#         # draw results\n",
    "#         for i, d in enumerate(detected):\n",
    "#             if genders[i] != 0:\n",
    "#                 number_male += 1\n",
    "#             label = \"{}, {}\".format(int(ages[i]), \"F\" if genders[i] == 0 else \"M\")\n",
    "#             draw_label(img, (d.left(), d.top()), label)\n",
    "        \n",
    "#         # Show number person and male/female\n",
    "#         number_person = len(detected)\n",
    "#         number_female = number_person - number_male\n",
    "# #         print(number_person)\n",
    "# #         print(\"Female {}\".format(number_female))\n",
    "# #         print(\"Male {}\".format(number_male))\n",
    "        \n",
    "#         # Display the results\n",
    "# #         for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "#         for (top, right, bottom, left), name in zip(detected, face_names):\n",
    "#             # Draw a box around the face\n",
    "#             cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "            \n",
    "#             # Draw a label with a name below the face\n",
    "#             cv2.rectangle(img, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#             font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#             cv2.putText(img, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "        \n",
    "#         cv2.putText(img, \"Male: {}\".format(number_male), (10, 30),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "#         cv2.putText(img, \"Female: {}\".format(number_female), (300, 30),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "# #         out.write(img)\n",
    "        \n",
    "#         cv2.imshow(\"result\", img)\n",
    "#         key = cv2.waitKey(1)\n",
    "\n",
    "#         if key == 27:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(model_path):\n",
    "    sess = tf.Session()\n",
    "    images_pl = tf.placeholder(tf.float32, shape=[None, 160, 160, 3], name='input_image')\n",
    "    images_norm = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame), images_pl)\n",
    "    train_mode = tf.placeholder(tf.bool)\n",
    "    age_logits, gender_logits, _ = inception_resnet_v1.inference(images_norm, keep_probability=0.8,\n",
    "                                                                 phase_train=train_mode,\n",
    "                                                                 weight_decay=1e-5)\n",
    "    gender = tf.argmax(tf.nn.softmax(gender_logits), 1)\n",
    "    age_ = tf.cast(tf.constant([i for i in range(0, 101)]), tf.float32)\n",
    "    age = tf.reduce_sum(tf.multiply(tf.nn.softmax(age_logits), age_), axis=1)\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\"restore model!\")\n",
    "    else:\n",
    "        pass\n",
    "    return sess,age,gender,train_mode,images_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, False, True, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "0.372330724106 \n",
      "[False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False]\n",
      "0.37136502422 damvantai1\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False]\n",
      "0.383846677805 damvantai31\n",
      "[False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False]\n",
      "0.366823706478 damvantai86\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, True, False, False, True, False, True, False, True, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False]\n",
      "0.361161828807 damvantai78\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, False, True, True, False, True, True, False, True, False, True, True, False, False, False, False, False, True, True, False, False, True, False, False, False, False]\n",
      "0.351941190321 damvantai4\n",
      "[False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, True, True, False, True, True, False, True, True, False, True, False, True, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False]\n",
      "0.375465951234 damvantai1\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, False, True, False, False, True, True, False, True, False, True, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False]\n",
      "0.37259811647 damvantai78\n",
      "[False, False, False, False, False, False, False, True, False, False, True, False, False, False, True, True, True, False, True, False, False, True, True, False, True, False, True, True, False, True, False, False, False, True, True, False, False, False, True, False, False, False]\n",
      "0.314024995661 damvantai1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-73795b550135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     args = parser.parse_args()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages_pl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgender\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages_pl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-be3b7ec310b8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(sess, age, gender, train_mode, images_pl)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m#         detect faces using dlib detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mdetected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_small_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--model_path\", \"--M\", default=\"./models\", type=str, help=\"Model Path\")\n",
    "#     args = parser.parse_args()\n",
    "    sess, age, gender, train_mode,images_pl = load_network(\"./models/\")\n",
    "    main(sess,age,gender,train_mode,images_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('/home/damvantai/Documents/face_recognition_demo/phodibo_hoankiem.mp4')\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
